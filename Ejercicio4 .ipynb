{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "EJERCICIO 4\n",
        "\n"
      ],
      "metadata": {
        "id": "qGGtwUMC2PQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlOxIocgz8sl",
        "outputId": "f524a3f2-c03d-48a3-d8cc-a57b621cbfe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Instalamos las librerías necesarias\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install kaggle\n",
        "\n",
        "# Importamos las librerías necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils import pad_sequences, to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Descomprimir el dataset\n",
        "!unzip /content/wiki_movie_plots_deduped.csv.zip\n",
        "\n",
        "# Cargar el dataset\n",
        "data = pd.read_csv('/content/wiki_movie_plots_deduped.csv.zip')\n",
        "\n",
        "# Mostrar las primeras filas del dataset\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsY9SQ692UOH",
        "outputId": "b2c9697d-5fad-45a0-b66c-c2a57e1ce9c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/wiki_movie_plots_deduped.csv.zip\n",
            "  inflating: wiki_movie_plots_deduped.csv  \n",
            "   Release Year                             Title Origin/Ethnicity  \\\n",
            "0          1901            Kansas Saloon Smashers         American   \n",
            "1          1901     Love by the Light of the Moon         American   \n",
            "2          1901           The Martyred Presidents         American   \n",
            "3          1901  Terrible Teddy, the Grizzly King         American   \n",
            "4          1902            Jack and the Beanstalk         American   \n",
            "\n",
            "                             Director Cast    Genre  \\\n",
            "0                             Unknown  NaN  unknown   \n",
            "1                             Unknown  NaN  unknown   \n",
            "2                             Unknown  NaN  unknown   \n",
            "3                             Unknown  NaN  unknown   \n",
            "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
            "\n",
            "                                           Wiki Page  \\\n",
            "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
            "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
            "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
            "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
            "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
            "\n",
            "                                                Plot  \n",
            "0  A bartender is working at a saloon, serving dr...  \n",
            "1  The moon, painted with a smiling face hangs ov...  \n",
            "2  The film, just over a minute long, is composed...  \n",
            "3  Lasting just 61 seconds and consisting of two ...  \n",
            "4  The earliest known adaptation of the classic f...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesar los datos\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Aplicar preprocesamiento a la columna de texto del dataset\n",
        "data['processed_text'] = data['Plot'].apply(preprocess_text)\n",
        "\n",
        "# Concatenar todas las oraciones en un solo corpus\n",
        "corpus = [word for sentence in data['processed_text'] for word in sentence]\n",
        "\n",
        "# Definir la longitud de la secuencia de entrenamiento\n",
        "train_len = 4\n",
        "\n",
        "# Crear las secuencias de texto\n",
        "text_sequences = []\n",
        "for i in range(train_len, len(corpus)):\n",
        "    seq = corpus[i-train_len:i]\n",
        "    text_sequences.append(seq)\n",
        "\n"
      ],
      "metadata": {
        "id": "XyeZFFvs2W9n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesar los datos\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "qXjg0-Us2ZF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar preprocesamiento a la columna de texto del dataset\n",
        "data['processed_text'] = data['Plot'].apply(preprocess_text)\n",
        "\n",
        "# Concatenar todas las oraciones en un solo corpus\n",
        "corpus = [word for sentence in data['processed_text'] for word in sentence]\n",
        "\n",
        "# Definir la longitud de la secuencia de entrenamiento\n",
        "train_len = 4\n",
        "\n"
      ],
      "metadata": {
        "id": "-RmrJLaw6oDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear las secuencias de texto\n",
        "text_sequences = []\n",
        "for i in range(train_len, len(corpus)):\n",
        "    seq = corpus[i-train_len:i]\n",
        "    text_sequences.append(seq)\n",
        "\n",
        "# Tokenizar las secuencias de texto\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(text_sequences)\n",
        "\n"
      ],
      "metadata": {
        "id": "kTle96GH6sPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las palabras a números\n",
        "sequences = tok.texts_to_sequences(text_sequences)\n",
        "\n",
        "# Convertir a numpy array\n",
        "arr_sequences = np.array(sequences)\n",
        "\n",
        "# Separar input y target\n",
        "x_data = arr_sequences[:,:-1]\n",
        "y_data_int = arr_sequences[:,-1]\n",
        "\n",
        "# Obtener el tamaño del vocabulario\n",
        "vocab_size = len(tok.word_counts) + 1\n"
      ],
      "metadata": {
        "id": "cDDa9sb46td3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transformar los datos a one hot encoding\n",
        "y_data = to_categorical(y_data_int, num_classes=vocab_size)\n",
        "\n",
        "# Crear el modelo\n",
        "input_seq_len = x_data.shape[1]\n",
        "output_size = vocab_size\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=input_seq_len))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "XjroDLDt6vWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Resumen del modelo\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "hist = model.fit(x_data, y_data, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "mzKv1ck36xGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Graficar los resultados\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count, y=hist.history['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count, y=hist.history['val_accuracy'], label='valid')\n",
        "plt.show()\n",
        "\n",
        "# Función para predecir la próxima palabra\n",
        "def model_response(human_text):\n",
        "    encoded = tok.texts_to_sequences([human_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=input_seq_len, padding='pre')\n",
        "    y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "    out_word = ''\n",
        "    for word, index in tok.word_index.items():\n",
        "        if index == y_hat:\n",
        "            out_word = word\n",
        "            break\n",
        "    return human_text + ' ' + out_word\n"
      ],
      "metadata": {
        "id": "-5qavTup6zOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prueba de la función de predicción\n",
        "print(model_response(\"The story of a young\"))\n",
        "\n",
        "# Generar secuencias nuevas\n",
        "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
        "    output_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == y_hat:\n",
        "                out_word = word\n",
        "                break\n",
        "        output_text += ' ' + out_word\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "l79alktU60lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prueba de la función de generación de secuencias\n",
        "input_text = \"A story about\"\n",
        "print(generate_seq(model, tok, input_text, max_length=3, n_words=5))\n",
        "\n",
        "# Conclusiones\n",
        "print(\"\\nConclusiones:\")\n",
        "print(\"El modelo entrenado puede predecir la próxima palabra en una secuencia dada. Sin embargo, el rendimiento del modelo podría mejorarse con más datos y más épocas de entrenamiento. Además, se pueden explorar arquitecturas más complejas para mejorar la precisión de las predicciones.\")\n"
      ],
      "metadata": {
        "id": "sB04BubM61_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}